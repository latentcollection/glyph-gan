{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "glyph-gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MJGUTylPs9C"
      },
      "source": [
        "# GlyphGAN\n",
        "Deep convolutional GAN trained on glyphs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hHt4tW8AbFa"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import imageio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFgcvcwsPoMc"
      },
      "source": [
        "## Download dataset from kaggle\n",
        "Store your kaggle.json file in your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iASXdWe5EkgF",
        "outputId": "60f5ab48-a22b-44de-ff03-6461b039af42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXknbZH6LuNy",
        "outputId": "0dea5ae8-aa1f-40e4-b690-f0d5a280474d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "# replace the first string with your file's location:\n",
        "%cp \"/content/gdrive/My Drive/kaggle.json\" \"/root/.kaggle/\"\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "# download your dataset:\n",
        "!kaggle datasets download font-book "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n",
            "Downloading font-book.zip to /content\n",
            " 40% 9.00M/22.3M [00:00<00:00, 58.9MB/s]\n",
            "100% 22.3M/22.3M [00:00<00:00, 88.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmosb7EcPP8Z"
      },
      "source": [
        "!unzip font-book.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt6b_thhJe2m"
      },
      "source": [
        "## Set–up model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfhXZSc9As4B"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, alpha=0.2):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        kernel_size = 4\n",
        "        padding = 1\n",
        "        stride = 2\n",
        "        \n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, kernel_size, stride, padding),\n",
        "            nn.LeakyReLU(alpha),\n",
        "            nn.Conv2d(128, 256, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(alpha),\n",
        "            nn.Conv2d(256, 512, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(alpha),\n",
        "            nn.Conv2d(512, 512, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(alpha),\n",
        "            nn.Conv2d(512, 512, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(alpha),\n",
        "            nn.Conv2d(512, 1024, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(alpha),\n",
        "        )\n",
        "        self.output = nn.Linear(4 * 4 * 1024, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        x = torch.reshape(x, (-1, 4 * 4 * 1024))\n",
        "        x = self.output(x)\n",
        "        \n",
        "        if self.training:\n",
        "            return x\n",
        "        \n",
        "        return F.sigmoid(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC99MNWQAzuA"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size=200, alpha=0.2):\n",
        "        super(Generator, self).__init__()       \n",
        "        kernel_size = 4\n",
        "        padding = 1\n",
        "        stride = 2\n",
        "        \n",
        "        self.input = nn.Linear(input_size, 4 * 4 * 1024)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(alpha),\n",
        "            nn.ConvTranspose2d(1024, 512, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(alpha),\n",
        "            nn.ConvTranspose2d(512, 512, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(alpha),\n",
        "            nn.ConvTranspose2d(512, 512, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(alpha),\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(alpha),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(alpha),\n",
        "            nn.ConvTranspose2d(128, 3, kernel_size, stride, padding),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "  \n",
        "    def forward(self, z):\n",
        "        x = self.input(z)\n",
        "        return self.net(x.view(-1, 1024, 4, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z3RLKorA2p-"
      },
      "source": [
        "class ImageFolderEX(datasets.ImageFolder):\n",
        "    def __getitem__(self, index):\n",
        "        def get_img(index):\n",
        "            path, label = self.imgs[index]\n",
        "            try:\n",
        "                img = self.loader(os.path.join(self.root, path))\n",
        "            except:\n",
        "                img = get_img(index + 1)\n",
        "            return img\n",
        "        img = get_img(index)\n",
        "        return self.transform(img) * 2 - 1  # rescale 0 - 1 to -1 - 1\n",
        "trans = transforms.Compose([\n",
        "    transforms.Resize((256, 256), interpolation=2), \n",
        "    transforms.ToTensor(), # implicitly normalizes the input to values between 0 - 1.\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NCNOIF_A9Cz"
      },
      "source": [
        "def train_dis(dis, gen, x):\n",
        "    z = torch.tensor(np.random.normal(0, 1, (batch_size, 200)), dtype=torch.float32)\n",
        "\n",
        "    if next(gen.parameters()).is_cuda:\n",
        "        x = x.cuda()\n",
        "        z = z.cuda()\n",
        "\n",
        "    dis.zero_grad()\n",
        "    y_real_pred = dis(x)\n",
        "    \n",
        "    idx = np.random.uniform(0, 1, y_real_pred.shape)\n",
        "    idx = np.argwhere(idx < 0.03)\n",
        "    \n",
        "    ones = np.ones(y_real_pred.shape) + np.random.uniform(-0.1, 0.1)\n",
        "    ones[idx] = 0\n",
        "    \n",
        "    zeros = np.zeros(y_real_pred.shape) + np.random.uniform(0, 0.2)\n",
        "    zeros[idx] = 1\n",
        "    ones = torch.from_numpy(ones).float()\n",
        "    zeros = torch.from_numpy(zeros).float()\n",
        "\n",
        "    if next(gen.parameters()).is_cuda:\n",
        "        ones = ones.cuda()\n",
        "        zeros = zeros.cuda()\n",
        "\n",
        "    loss_real = F.binary_cross_entropy_with_logits(y_real_pred, ones)\n",
        "\n",
        "    generated = gen(z)\n",
        "    y_fake_pred = dis(generated)\n",
        "\n",
        "    loss_fake = F.binary_cross_entropy_with_logits(y_fake_pred, zeros)\n",
        "    loss = loss_fake + loss_real\n",
        "    loss.backward()\n",
        "    optimizer_dis.step()\n",
        "    return loss\n",
        "\n",
        "            \n",
        "def train_gen(gen, batch_size):\n",
        "    z = torch.tensor(np.random.normal(0, 1, (batch_size, 200)), dtype=torch.float32)\n",
        "    \n",
        "    if next(gen.parameters()).is_cuda:\n",
        "        z = z.cuda()\n",
        "    \n",
        "    gen.zero_grad()\n",
        "    generated = gen(z)\n",
        "    y_fake = dis(generated)\n",
        "\n",
        "    ones = torch.ones_like(y_fake)\n",
        "    if next(gen.parameters()).is_cuda:\n",
        "        ones = ones.cuda()\n",
        "\n",
        "    loss = F.binary_cross_entropy_with_logits(y_fake, ones)\n",
        "    loss.backward()\n",
        "    optimizer_gen.step()\n",
        "    return loss, generated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoZ7luZZJkDS"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23geH2o1SIOk"
      },
      "source": [
        "'''\n",
        "set to your database's directory.\n",
        "divide data classes into resp. subdirs.\n",
        "see readme for further information.\n",
        "'''\n",
        "img_dir = \"/root/a/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKwIahp4dzVw"
      },
      "source": [
        "def genTestImg():\n",
        "  x = torch.tensor(np.random.normal(0, 1, (batch_size, 200)), dtype=torch.float32)\n",
        "  img = gen(x.cuda())\n",
        "  img = img.cpu().detach().numpy()[0]\n",
        "  img = np.transpose(img, (1,2,0))\n",
        "  img = np.uint8(np.interp(img, (-1, 1), (0, 255)))\n",
        "  plt.imshow(img)\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTdkvKtLBeM4"
      },
      "source": [
        "dis = Discriminator().cuda()\n",
        "gen = Generator().cuda()\n",
        "\n",
        "lr = 0.0002 # default 0.0002\n",
        "beta_1 = 0.5 # default 0.5\n",
        "beta_2 = 0.999 # default 0.999\n",
        "optimizer_gen = torch.optim.Adam(gen.parameters(), lr, betas=(beta_1, beta_2))\n",
        "optimizer_dis = torch.optim.Adam(dis.parameters(), lr, betas=(beta_1, beta_2))\n",
        "\n",
        "epochs = 50 # default 30\n",
        "batch_size = 64 # default 64\n",
        "data = torch.utils.data.DataLoader(ImageFolderEX(img_dir, trans), \n",
        "\t\t\t\t   batch_size=batch_size, shuffle=True, \n",
        "\t\t\t\t   drop_last=True, num_workers=2)\n",
        "\n",
        "n = len(data)\n",
        "for epoch in range(0, epochs):\n",
        "    c = 0\n",
        "    n = len(data) \n",
        "\n",
        "    for x in iter(data): \n",
        "        c += 1\n",
        "\n",
        "        loss_dis = train_dis(dis, gen, x)\n",
        "        loss_gen, generated = train_gen(gen, batch_size)\n",
        "        \n",
        "        global_step = epoch * n + c\n",
        "\n",
        "        print(f'{c} \\t loss_dis: {loss_dis.item()} \\t loss_gen: {loss_gen.item()} \\t epoch: {epoch}, \\t global_step: {c}/{n}')\n",
        "        genTestImg() # comment this out if you don't want to generate test imgs every iteration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd20AO98PdAM"
      },
      "source": [
        "## Run model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMkg4TLZBgdh"
      },
      "source": [
        "# generates 8 random tensors:\n",
        "x1 = torch.tensor(np.random.normal(0, 1, (batch_size, 200)), dtype=torch.float32)\n",
        "x2 = torch.tensor(np.random.normal(0, 1, (batch_size, 200)), dtype=torch.float32)\n",
        "x3 = torch.tensor(np.random.normal(0, 1, (batch_size, 200)), dtype=torch.float32)\n",
        "x4 = torch.tensor(np.random.normal(0, 1, (batch_size, 200)), dtype=torch.float32)\n",
        "x5 = torch.tensor(np.random.normal(0, 1, (batch_size, 200)), dtype=torch.float32)\n",
        "x6 = torch.tensor(np.random.normal(0, 1, (batch_size, 200)), dtype=torch.float32)\n",
        "x7 = torch.tensor(np.random.normal(0, 1, (batch_size, 200)), dtype=torch.float32)\n",
        "x8 = torch.tensor(np.random.normal(0, 1, (batch_size, 200)), dtype=torch.float32)\n",
        "\n",
        "# generates images based using above tensors:\n",
        "sett = [x1,x2,x3,x4,x5,x6,x7,x8,x1]\n",
        "for i in sett:\n",
        "  img2 = gen(i.cuda())\n",
        "  img2 = img2.cpu().detach().numpy()[0]\n",
        "  img2 = np.transpose(img2, (1,2,0))\n",
        "  img2 = np.uint8(np.interp(img2, (-1, 1), (0, 255)))\n",
        "  plt.imshow(img2)\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kruDvhupHwvu"
      },
      "source": [
        "## Post–processing\n",
        "Renders a linearly interpolated latent space video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE7xDhpt7_4p"
      },
      "source": [
        "latentspace = []\n",
        "\n",
        "# choose the order in which you want the images to appear in the interpolation video\n",
        "sett = [x6,x3,x8,x2,x4,x5,x1,x7,x6]\n",
        "\n",
        "# this generates the interpolated framesusing linear interpolation\n",
        "# the frames variable decide the number of images between two initial tensors:\n",
        "frames = 100\n",
        "for p in range(len(sett)-1):\n",
        "  for k in range(frames):\n",
        "    latent = sett[p].clone()\n",
        "    for c in range(8):\n",
        "      for i in range(200):\n",
        "        newvalue = np.linspace(sett[p][c][i], sett[p+1][c][i],frames)[k]\n",
        "        latent[c][i] = newvalue\n",
        "    img2 = gen(latent.cuda())\n",
        "    img2 = img2.cpu().detach().numpy()[0]\n",
        "    img2 = np.transpose(img2, (1,2,0))\n",
        "    img2 = np.uint8(np.interp(img2, (-1, 1), (0, 255)))\n",
        "    latentspace.append(img2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2tZ5ZYz_ve9"
      },
      "source": [
        "# combines the generated latent space images into a video file\n",
        "imageio.mimwrite(\"glyphgan-render.mp4\", latentspace , fps = 30)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}